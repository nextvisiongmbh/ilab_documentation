
# Nächste Herausforderungen
 
- **Checkpointing**: Aktuell scheint es nur die Möglichkeit zu geben, nach jeder Trainings-Epoche das Modell zwischenzuspeichern oder gar nicht. Wenn ich nach jeder Epoche speichere, reicht allerdings der Speicher nicht, weil das Modell riesig ist. Ich versuche gerade rauszubekommen, ob was dazwischen möglich ist.
- **Multiple LoRa-Adapters (Geringe Prio)**: Ich informiere mich nebenbei, ob und wie es möglich ist, mit InstructLab auf die einzelnen LoRa-Adapter zuzugreifen, damit man mehrere fine-tunings in einem Modell effizient unterbringen kann
- **Testen**: Da hatte uns Max schon mal eine Plattform empfohlen, habe ich aber leider nicht notiert. Weiß das noch einer oder hat eine Empfehlung?
- **Evaluation**: Die Evaluation von verschiedenen fine-getuneten Modellen mit Hilfe von eine Judge-Modell. Sowohl verstehen, als auch zum Laufen bekommen
- **Deutsche Trainingssätze**...
